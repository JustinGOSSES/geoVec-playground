{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick exploration of geovec, a pre-trained word embedding for geology in gloVe format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium article by authors: https://towardsdatascience.com/geovec-word-embeddings-for-geosciences-ac1e1e854e19\n",
    "\n",
    "NOT ME!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CITATION\n",
    "@misc{padarian2019geovec,\n",
    "  title={GeoVec},\n",
    "  url={https://osf.io/4uyeq},\n",
    "  DOI={10.17605/OSF.IO/4UYEQ},\n",
    "  publisher={OSF},\n",
    "  author={Padarian, José and Fuentes, Ignacio},\n",
    "  year={2019}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article\n",
    "@misc{padarian2019word,\n",
    "  title={Word embeddings for application in geosciences: development, evaluation and examples of soil-related concepts},\n",
    "  url={https://doi.org/10.5194/soil-2018-44}\n",
    "  DOI={10.5194/soil-2018-44},\n",
    "  publisher={Copernicus GmbH},\n",
    "  author={Padarian, José and Fuentes, Ignacio},\n",
    "  year={2019},\n",
    "  journal={SOIL Discuss}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data file of the pretrained embedding is downloaded from these links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download site & information https://osf.io/4uyeq/wiki/home/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h5 file page = https://osf.io/4uyeq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the work vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geovec_file_url = \"https://osf.io/kt6eu/download\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This download will take a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geovec_r = requests.get(geovec_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This download will take a couple of minutes too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('geovec_300d_v1.h5', 'wb') as file:\n",
    "    file.write(geovec_r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "##### glove python is a dependency mentioend by the authors\n",
    "##### DON'T DO THIS AS IT HAS AlL SORTS OF PROBLEMS ON MACOS AND WINDOWS\n",
    "\n",
    "##### Specifically DO NOT DO `!pip install glove_python` <-- This is only helper code which we can do other ways. This  repository is no longer maintained and causes problems on MacOS and Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of using glove-python, we can just open the file this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('geovec_300d_v1.h5', 'r') as f:\n",
    "        v = np.zeros(f['vectors'].shape, f['vectors'].dtype)\n",
    "        f['vectors'].read_direct(v)\n",
    "        dct = f['dct'][()].tostring().decode('utf-8')\n",
    "        dct = json.loads(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words with vector embeddings 411845\n",
      "number of vectors in each embedding 300\n"
     ]
    }
   ],
   "source": [
    "#### number of words with vector embeddings\n",
    "print(\"number of words with vector embeddings\",len(v))\n",
    "#### Length of vectors\n",
    "print(\"number of vectors in each embedding\",len(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words total 411845\n",
      "and what each word in dictionary looks like:  acrodermatitis : 391621\n"
     ]
    }
   ],
   "source": [
    "print(\"number of words total\",len(dct))\n",
    "print(\"and what each word in dictionary looks like: \",list(dct.keys())[0:1][0],\":\",dct[list(dct.keys())[0:1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for acrodermatitis [ 2.14973819e-02  3.54598160e-03  4.77534719e-02  4.78067286e-02\n",
      "  1.17590288e-02  2.33181473e-02 -3.30265872e-02  1.04145538e-02\n",
      "  2.94735283e-02  1.76292490e-02  3.28493491e-02 -5.50193898e-02\n",
      " -4.42245863e-02  1.82517376e-02  9.81923658e-03  2.50976533e-02\n",
      " -2.15862114e-02  2.29525547e-02  3.83460931e-02  6.62240991e-03\n",
      " -2.28440519e-02 -3.59909981e-03 -3.00370604e-02  7.05488492e-03\n",
      " -1.05769737e-02  2.60596797e-02  3.80290374e-02  1.60412807e-02\n",
      " -1.29007231e-02  4.02748212e-02 -1.60112903e-02  2.54279445e-03\n",
      " -1.62100010e-02  4.09765076e-03 -1.30509147e-02  1.35210659e-02\n",
      " -2.27812286e-02 -6.74992893e-03 -3.53130512e-04  4.24857810e-02\n",
      "  4.36526351e-02  2.17911061e-02  1.93313900e-02 -2.29305774e-02\n",
      " -3.98880281e-02 -9.33703594e-03  3.27533185e-02  5.75091690e-02\n",
      "  3.55911404e-02  3.56697030e-02 -1.93229970e-02 -7.98947364e-03\n",
      "  7.19980756e-03  3.01554874e-02  1.18414760e-02  2.21565347e-02\n",
      "  1.39588956e-02  2.50078849e-02  2.77456548e-02 -2.04478782e-02\n",
      "  9.29796952e-04 -6.58220705e-03  1.46599449e-02  1.22692809e-02\n",
      " -1.20404800e-02  3.89050059e-02  2.59135338e-03  4.98056673e-02\n",
      "  7.02090678e-04 -1.59686077e-02  3.17037851e-02 -1.66921169e-02\n",
      "  2.12845169e-02  3.23234336e-03 -9.68716014e-03  2.08827294e-02\n",
      " -2.20393315e-02 -1.60487350e-02 -6.51785173e-03 -5.13372682e-02\n",
      " -1.24659296e-02  1.43541750e-02  1.96209382e-02  1.19900974e-02\n",
      " -5.66662289e-03 -2.83928551e-02 -2.13999320e-02 -1.98144112e-02\n",
      "  1.78322587e-02  3.61205935e-02  1.81435887e-02 -4.40385602e-02\n",
      "  1.01028839e-02  5.43047022e-03 -1.65896509e-02 -6.04715606e-04\n",
      " -2.08751131e-02 -2.51951092e-03  1.59743577e-02 -1.82590783e-02\n",
      "  3.62116359e-02  2.30260976e-02  6.11918792e-03  2.83546560e-02\n",
      "  3.63721587e-02  2.13112459e-02  8.09391495e-03  2.66113896e-02\n",
      " -4.25905135e-04  4.44074720e-02  3.08294687e-03 -3.04689296e-02\n",
      "  2.03943495e-02 -1.59237292e-02  2.01299135e-02 -2.18885820e-02\n",
      "  5.27934870e-03 -3.05727068e-02  4.41627996e-03 -3.02263685e-02\n",
      " -2.03108266e-02  1.91632267e-02 -1.53189981e-02 -2.88745034e-02\n",
      " -5.83090633e-02  1.52602568e-02  1.17646425e-03 -2.81848875e-03\n",
      " -1.77240316e-02 -4.84348927e-03 -2.89313849e-02 -1.30472118e-02\n",
      " -6.34984253e-03  2.23114975e-02  1.05271460e-02  7.09842891e-04\n",
      " -1.61968730e-02 -7.30716391e-03  2.35040151e-02  1.01555195e-02\n",
      "  2.70135980e-02  2.64211558e-02 -1.51980808e-02  3.48691940e-02\n",
      "  1.42039303e-02  2.60456186e-03 -3.95941176e-02  5.34166396e-03\n",
      "  4.87501137e-02 -4.96817520e-03 -2.53795162e-02  1.18946647e-02\n",
      " -1.01929540e-02 -1.53890178e-02 -1.76931806e-02  4.18040482e-03\n",
      "  2.41322797e-02 -1.14066470e-02  6.12265011e-03  1.95911173e-02\n",
      "  2.77570840e-02  1.99424592e-03 -7.40068406e-03 -4.94151786e-02\n",
      "  2.90932748e-02  1.19669698e-02  8.70522484e-03 -2.24969592e-02\n",
      " -2.02495903e-02 -4.52310815e-02 -3.26838065e-03 -2.79623223e-03\n",
      "  3.42045724e-03 -4.32943366e-02 -2.92485394e-02  1.92005895e-02\n",
      " -8.38350784e-03  8.47931486e-03 -7.38260290e-03  4.01896378e-03\n",
      "  4.28952556e-03 -1.43073164e-02  1.68694351e-02 -9.73304920e-03\n",
      "  1.56094006e-03  2.12623198e-02 -1.84264034e-02  1.97724369e-03\n",
      "  2.27683429e-02 -1.05288029e-02 -2.94228606e-02 -1.28255878e-02\n",
      " -2.85087861e-02 -2.23932671e-03  2.80051641e-02 -2.31999643e-02\n",
      " -6.20532874e-03  5.24375914e-03 -9.87625681e-03  5.06147929e-02\n",
      " -1.23001691e-02 -3.83031331e-02 -1.94737539e-02 -6.41896436e-03\n",
      " -1.77073479e-02 -4.78789583e-02 -1.07676741e-02 -3.20272110e-02\n",
      "  1.88208707e-02 -1.94697157e-02 -2.48955339e-02 -8.84655584e-03\n",
      "  4.25807647e-02  2.24879906e-02  3.45966616e-03  1.50735974e-02\n",
      " -1.40801882e-02 -9.65681579e-03 -1.47380657e-03 -3.34318951e-02\n",
      "  2.80054267e-02  2.60639954e-02  2.21503135e-02 -1.49941556e-02\n",
      "  1.24347778e-02  2.32335925e-02  2.70025413e-02 -1.34797059e-02\n",
      " -1.08819501e-02 -2.19717938e-02 -1.67791042e-02  4.25624810e-02\n",
      "  1.06274961e-02 -3.50405723e-02  6.65106345e-04  2.24141497e-02\n",
      " -2.08849292e-02 -3.41290198e-02  3.20449173e-02 -9.15627345e-04\n",
      "  6.70802593e-02  1.26132220e-02 -3.07596624e-02  3.88195913e-04\n",
      "  3.03201657e-02  6.21471182e-03 -2.70464309e-02  2.08262745e-02\n",
      "  1.59443226e-02 -1.07012615e-02  2.68454142e-02 -1.40027795e-02\n",
      "  3.49567607e-02 -3.41518708e-02  3.26485597e-02  1.20633161e-02\n",
      "  3.35888704e-04 -4.04036324e-03  5.34426374e-03 -2.19369214e-02\n",
      " -2.39136294e-02 -6.83886884e-03 -3.69636714e-02 -1.99336801e-02\n",
      " -7.38115935e-03  8.03342834e-03 -2.64239442e-02  2.96763796e-02\n",
      "  3.36564295e-02  9.23472643e-03  1.44549524e-02  7.06988340e-03\n",
      " -2.91814897e-02  2.36636624e-02  4.71610622e-03  5.53552643e-04\n",
      "  4.96259145e-03 -1.94699969e-02  1.74889490e-02 -1.00082997e-02\n",
      " -2.08951645e-02 -4.12994996e-02 -1.17933042e-02 -1.54148871e-02\n",
      " -3.33946943e-02  1.98158473e-02  2.51641236e-02 -5.29359607e-03\n",
      "  5.26997307e-03 -7.25202626e-05 -5.35767972e-02 -1.19735682e-02\n",
      " -6.31901901e-03 -2.38375254e-02 -3.35687175e-02 -2.85896584e-02\n",
      "  1.01019340e-02 -4.22223322e-02 -5.82430623e-02  2.82554049e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding for acrodermatitis\",v[dct[list(dct.keys())[0:1][0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acrodermatitis',\n",
       " 'groenlandica',\n",
       " 'microbaculatus',\n",
       " 'typotheriopsis',\n",
       " 'cvesicle',\n",
       " 'epss',\n",
       " 'mników',\n",
       " 'istin',\n",
       " 'vlvg',\n",
       " 'heatflux']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dct.keys())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll combine the two files into a single dictionary to make some later things a bit easier. It isn't saved in thi format to save space according to the authors who trained it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceIntsWithWords(vectors,dictionary):\n",
    "    words = list(dct.keys())\n",
    "    embeddings_dict = {}\n",
    "    for word in words:\n",
    "        embeddings_dict[word] = np.asarray(v[dictionary[word]][:],\"float32\")\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vector_embedding = replaceIntsWithWords(v,dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03063519, -0.41190132, -0.01748249, -0.03562951, -0.44922313,\n",
       "       -0.23300318,  0.22224723, -0.07885347,  0.02758265, -0.07660752,\n",
       "       -0.39882764,  0.19357805, -0.03862172, -0.16392523, -0.08809105,\n",
       "       -0.15323128, -0.17489772,  0.14711891,  0.10426124, -0.02329986,\n",
       "        0.09977868,  0.23435533,  0.3037015 , -0.6652086 , -0.1779219 ,\n",
       "        0.16997509, -0.0749488 , -0.4623773 , -0.18799299, -0.04015233,\n",
       "       -0.1230348 , -0.324141  ,  0.32538635,  0.27751258, -0.04844321,\n",
       "        0.04106978,  0.3250498 ,  0.4131149 ,  0.41197386,  0.05734786,\n",
       "       -0.28493047,  0.05330373, -0.30320197, -0.6228066 ,  0.17232999,\n",
       "        0.13726279,  0.02862607,  0.00987866, -0.22104132, -0.27486506,\n",
       "        0.3875012 ,  0.05712619,  0.30534878, -0.17705044, -0.19142899,\n",
       "       -0.1692242 , -0.3759296 ,  0.35963646, -0.02505949,  0.05952422,\n",
       "        0.33059987,  0.18033917,  0.05865888,  0.1908917 ,  0.03191795,\n",
       "       -0.3452011 , -0.4242747 ,  0.02242155, -0.08702113, -0.36016336,\n",
       "       -0.09096049, -0.15263985, -0.10395551,  0.3015002 , -0.19887997,\n",
       "        0.17470044,  0.4872355 ,  0.17027885,  0.1629167 ,  0.2500491 ,\n",
       "        0.0590988 , -0.30714718,  0.33032542, -0.54410225,  0.2931073 ,\n",
       "       -0.0657116 , -0.0493247 , -0.04416075,  0.23244466, -0.4811705 ,\n",
       "       -0.11674033,  0.5065609 , -0.23595892, -0.40718448,  0.03079501,\n",
       "       -0.02404374, -0.05897579, -0.40997535, -0.29971278, -0.06485663,\n",
       "       -0.0735241 , -0.57915825,  0.04794372,  0.15152007, -0.38089737,\n",
       "        0.04072325, -0.0973587 , -0.281366  , -0.0866443 ,  0.07327744,\n",
       "       -0.1289843 , -0.03622092, -0.16641869, -0.02401751,  0.01719305,\n",
       "       -0.18703395,  0.36526188,  0.3011297 ,  0.10935438,  0.2580626 ,\n",
       "        0.34569156, -0.14707701,  0.26744878, -0.23805387, -0.06046972,\n",
       "        0.43298253, -0.38266245, -0.2280435 , -0.09001176,  0.22924648,\n",
       "        0.01707637, -0.21080957, -0.2534994 , -0.20905982, -0.31777838,\n",
       "        0.02439837,  0.18417667,  0.14000054, -0.15615216, -0.2249662 ,\n",
       "       -0.02894812, -0.33743653, -0.41191328, -0.05852659, -0.33302596,\n",
       "        0.04976951,  0.5980457 , -0.04356075, -0.18495524,  0.20690395,\n",
       "        0.05941332, -0.32292056,  0.17914008, -0.500527  , -0.26573032,\n",
       "       -0.08153787,  0.12664302, -0.33359897,  0.25827253,  0.1932546 ,\n",
       "        0.24806926, -0.10873552, -0.4112353 , -0.48139846, -0.24745034,\n",
       "        0.4930274 ,  0.20221478,  0.1466965 ,  0.27478045,  0.08422571,\n",
       "        0.0078306 ,  0.00467938,  0.12200137,  0.38119838,  0.63320845,\n",
       "       -0.23246406,  0.13708606, -0.07137589, -0.06196122, -0.3991872 ,\n",
       "       -0.16195753,  0.39561355,  0.1375364 , -0.34327468,  0.160251  ,\n",
       "        0.2830336 , -0.0162445 ,  0.4052586 , -0.11772639,  0.08404472,\n",
       "       -0.07816593, -0.04536095, -0.15027593, -0.07181185, -0.40633082,\n",
       "       -0.13952276,  0.1901891 ,  0.46117383, -0.04458065, -0.4252203 ,\n",
       "       -0.36120528,  0.30159628,  0.39013577,  0.0528173 , -0.12946293,\n",
       "        0.3917904 , -0.16849914, -0.24559385, -0.20524621, -0.00520904,\n",
       "        0.02467151,  0.3680092 , -0.13684408, -0.03159561, -0.17883603,\n",
       "       -0.21492326,  0.53078973,  0.33911052,  0.09338158, -0.23546411,\n",
       "       -0.09534077,  0.2798333 ,  0.11799586, -0.351914  , -0.06308249,\n",
       "        0.11057905, -0.12907316,  0.07775687,  0.03375599,  0.22878551,\n",
       "       -0.19734228, -0.15298705,  0.03418294, -0.1719191 ,  0.23001723,\n",
       "       -0.23243229,  0.16741559,  0.05114592, -0.06052561, -0.46340165,\n",
       "       -0.46262592, -0.6035481 ,  0.16142583, -0.48702064, -0.19704111,\n",
       "       -0.46407148, -0.05907227, -0.17363416, -0.04588016,  0.08782403,\n",
       "        0.45838422, -0.01058303, -0.36841273,  0.03393587, -0.00892907,\n",
       "        0.01773651, -0.02713825, -0.18003365, -0.23333898, -0.23033081,\n",
       "        0.22356518, -0.19672231,  0.07019778,  0.34114864,  0.21038549,\n",
       "       -0.01267198, -0.0917694 , -0.5532432 , -0.21704294, -0.51371676,\n",
       "        0.04314219,  0.05385855, -0.20565934, -0.12910867, -0.38981983,\n",
       "       -0.22497126,  0.25124222,  0.10278568, -0.34534988, -0.13590793,\n",
       "        0.43126598,  0.22235622, -0.29292405, -0.01442697,  0.19515485,\n",
       "        0.08376491, -0.4344467 , -0.12742433, -0.17324965, -0.12557042,\n",
       "        0.13236505,  0.29445967,  0.02982007, -0.22455895,  0.16065286,\n",
       "       -0.20524757,  0.08688244,  0.22455838,  0.11694988, -0.00187241],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vector_embedding[\"sandstone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's play around a bit to find out what words are close to others in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embeddings_dict,embedding_vector):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411845"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vector_embedding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.911217212677002"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.euclidean(new_vector_embedding['sandstone'], new_vector_embedding['granite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sandstone',\n",
       " 'siltstone',\n",
       " 'mudstone',\n",
       " 'arkosic',\n",
       " 'conglomerate',\n",
       " 'rothbach',\n",
       " 'gröden',\n",
       " 'sonta',\n",
       " 'mudstones',\n",
       " 'interbedded']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(new_vector_embedding,new_vector_embedding[\"sandstone\"])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['granite',\n",
       " 'granitic',\n",
       " 'ebat',\n",
       " 'tuaní',\n",
       " 'kibe',\n",
       " 'lavadores',\n",
       " 'baerzhe',\n",
       " 'daguzhai',\n",
       " 'cooyerdoo',\n",
       " 'qôrqut']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(new_vector_embedding,new_vector_embedding[\"granite\"])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crossbedding',\n",
       " 'lumi',\n",
       " 'unrein',\n",
       " 'superface',\n",
       " 'boundinaged',\n",
       " 'staretsky',\n",
       " 'essikado',\n",
       " 'gerlarch',\n",
       " 'doucette',\n",
       " 'szenleng',\n",
       " 'rhune',\n",
       " 'shihui',\n",
       " 'gackowa',\n",
       " 'lenticularity',\n",
       " 'tenappera',\n",
       " 'karlshafen',\n",
       " 'håkjerringdjupet',\n",
       " 'anorbital',\n",
       " 'eludicate',\n",
       " 'torom']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_embeddings(new_vector_embedding,new_vector_embedding[\"crossbedding\"])[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like this is a lot of non-English words?!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try subtraction of pure vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sagyeri', 'pregrowth', 'barzaloza', 'translatent', 'syntransform', 'dothan', 'superincumbent', 'corniola', 'adoudou', 'chingmei', 'březno', 'saberian', 'kulm', 'trentinara', 'flamy', 'chulca', 'setlaole', 'emborozu', 'ikpe', 'guxu', 'aib', 'sarsbukta', 'huilu', 'nunda', 'lecho']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(new_vector_embedding,\n",
    "    new_vector_embedding[\"stratum\"] - new_vector_embedding[\"fault\"]\n",
    ")[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marble', 'sedef', 'pentelic', 'supren', 'macael', 'yeosan', 'calanchiole', 'pentelikon', 'kemalpasa', 'dionysos', 'borba', 'alabashka', 'iscehisar', 'maniçoba', 'hekkelstrand', 'benou', 'calcsilicates', 'evenes', 'harsin', 'montepuez', 'ayhan', 'nanshu', 'sanchuan', 'belevi', 'bettadabidu']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(new_vector_embedding,\n",
    "    new_vector_embedding[\"marble\"] - new_vector_embedding[\"heat\"]\n",
    ")[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intratill', 'satitoa', 'boulder', 'montoz', 'laxe', 'kintradwell', 'unclimbed', 'settai', 'norber', 'alveolized', 'intertill', 'cipit', 'preexposure', 'kudaka', 'moutonnee', 'ventifacted', 'navabandar', 'sturgis', 'taufererberg', 'pineo', 'preexposed', 'mathu', 'runswick', 'spelonk', 'encierro']\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_embeddings(new_vector_embedding,\n",
    "    new_vector_embedding[\"boulder\"] - new_vector_embedding[\"clay\"]\n",
    ")[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A blunt approach doesn't work so well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is a bit hard to see how well this works due to the number of words in other languages and placenames I don't know.... so let's limit it to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/dwyl/english-words/master/words_dictionary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_to_eng_words_as_json = \"https://raw.githubusercontent.com/dwyl/english-words/master/words_dictionary.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_words_as_json_r  = requests.get(url_to_eng_words_as_json )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_words_as_json = eng_words_as_json_r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'aa', 'aaa']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(eng_words_as_json.keys())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370101"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_words_as_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411845"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vector_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limitGeovecEmbeddingToEnglishWords(new_vector_embedding,eng_words_as_json):\n",
    "    english_words = list(eng_words_as_json.keys())\n",
    "    new_english_only_embedding = {}\n",
    "    for word in english_words:\n",
    "        if word in new_vector_embedding:\n",
    "            new_english_only_embedding[word] = new_vector_embedding[word]\n",
    "    return new_english_only_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkIfWordsExistInEmbedding(embedding,words):\n",
    "    embedding_words = list(embedding.keys())\n",
    "    for word in words:\n",
    "        if(word in embedding_words):\n",
    "            print(word,\" exists in embedding\")\n",
    "        else:\n",
    "            print(word,\" DOES NOT EXIST IN EMBEDDING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_I_think_should_be_there = [\"sandstone\",\"limestone\",\"bed\",\"fault\",\"fracture\",\"volcano\",\"tuff\",\"strike\",\"dip\",\"line\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vector_embedding_english_only2 = limitGeovecEmbeddingToEnglishWords(new_vector_embedding,eng_words_as_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88626"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vector_embedding_english_only2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandstone  exists in embedding\n",
      "limestone  exists in embedding\n",
      "bed  exists in embedding\n",
      "fault  exists in embedding\n",
      "fracture  exists in embedding\n",
      "volcano  exists in embedding\n",
      "tuff  exists in embedding\n",
      "strike  exists in embedding\n",
      "dip  exists in embedding\n",
      "line  exists in embedding\n"
     ]
    }
   ],
   "source": [
    "checkIfWordsExistInEmbedding(new_vector_embedding_english_only2,words_I_think_should_be_there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acmite',\n",
       " 'acne',\n",
       " 'acneiform',\n",
       " 'acnida',\n",
       " 'acoela',\n",
       " 'acolyte',\n",
       " 'acoma',\n",
       " 'aconite',\n",
       " 'aconitic',\n",
       " 'aconitum',\n",
       " 'acop',\n",
       " 'acor',\n",
       " 'acorn',\n",
       " 'acorus',\n",
       " 'acoupa',\n",
       " 'acoustic',\n",
       " 'acoustical',\n",
       " 'acoustically',\n",
       " 'acoustician',\n",
       " 'acoustoelectric']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(new_vector_embedding_english_only2.keys())[710:730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88626"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_vector_embedding_english_only2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much better result!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's remake the vector and metadata for the embedding projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeVectorTSV(embeddingVectorsOnly):\n",
    "    out_v = io.open('log/vecs.tsv', 'w', encoding='utf-8')\n",
    "    for n,weights in enumerate(embeddingVectorsOnly):\n",
    "        vec = embeddingVectorsOnly[n]\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMetaTSV(words):\n",
    "    with open('log/metadata1.tsv', 'w') as f:\n",
    "#     for word in word2vec.words:\n",
    "        print(len(words),\"length of words\")\n",
    "        for word in words:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeEmbeddingWithWordsAndVectorsToFilesForEmbeddingProjector(embedding):\n",
    "    vectors_only = []\n",
    "    keys_in_array = list(embedding.keys())\n",
    "    for key in keys_in_array:\n",
    "        vectors_only.append(embedding[key])\n",
    "    makeVectorTSV(vectors_only)\n",
    "    makeMetaTSV(keys_in_array)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88626 length of words\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "writeEmbeddingWithWordsAndVectorsToFilesForEmbeddingProjector(new_vector_embedding_english_only2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although you can start up the embedding projector - which is part of tensorboard from Jupyter lab, the way all the tutorials show starting it assume training of a model has gone on. Since we aren't doing that, they don't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could just go to the link to the version on the web here: http://projector.tensorflow.org/ and load your two TSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the easiest solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we'll copy the repo that is used to make that page and exclude the git files so it doesn't mess with the git repository that this notebook is under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_along_r = requests.get(\"https://github.com/tensorflow/embedding-projector-standalone/archive/master.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: embedding-projector-standalone: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir embedding-projector-standalone\n",
    "!cd embedding-projector-standalone\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('embedding-projector-standalone.zip', 'wb') as file:\n",
    "    file.write(stand_along_r.content)\n",
    "!mv embedding-projector-standalone.zip embedding-projector-standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('embedding-projector-standalone/embedding-projector-standalone.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall('embedding-projector-standalone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...\n",
      "127.0.0.1 - - [29/Jul/2020 22:40:45] \"GET /embedding-projector-standalone/embedding-projector-standalone-master/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Jul/2020 22:40:46] \"GET /embedding-projector-standalone/embedding-projector-standalone-master/oss_data/oss_demo_projector_config.json HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Jul/2020 22:40:46] code 404, message File not found\n",
      "127.0.0.1 - - [29/Jul/2020 22:40:46] \"GET /embedding-projector-standalone/favicon.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [29/Jul/2020 22:40:46] \"GET /embedding-projector-standalone/embedding-projector-standalone-master/oss_data/vecs.tsv HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Jul/2020 22:40:55] \"GET /embedding-projector-standalone/embedding-projector-standalone-master/oss_data/metadata1.tsv HTTP/1.1\" 200 -\n",
      "^C\n",
      "\n",
      "Keyboard interrupt received, exiting.\n"
     ]
    }
   ],
   "source": [
    "! cd embedding-projector-standalone/embedding-projector-standalone-master\n",
    "!python -m http.server\n",
    "!cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now open a browser and go to: http://0.0.0.0:8000/ or http://0.0.0.0:8000/embedding-projector-standalone/embedding-projector-standalone-master/\n",
    "\n",
    "You may have to navigate down to get to embedding-projector-standalone/embedding-projector-standalone-master/index.html\n",
    "\n",
    "Once there, you should see something that looks like this: http://projector.tensorflow.org/  as the page you've opened is built from the same repository.\n",
    "\n",
    "On the lefthand side, you should see \"load\" button. Click that and select your vectors TSV file and then select your metadata TSV file. \n",
    "\n",
    "You should see your embedded space of geology related terms loaded into the projector view!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see the nearest neighbors to a word:\n",
    "Look at the right sidebar and find the \"search\" input. Put in a geology related word like \"limestone\". You should be presented with related words. If you click one of them, you can select to see the 101 closely related terms only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or, I might have already created these files and they're already in this repository and available as a github pages page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stand alone embedding projector comes with a bunch of demo datasets that are quite large and will gum your git if you try to include them, so you might want to delet the ones you don't need with the commeand below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: embedding-projector-standalone/embedding-projector-standalone-master/oss_data/word2vec*: No such file or directory\n",
      "rm: embedding-projector-standalone/embedding-projector-standalone-master/oss_data/bnmt_enjako*: No such file or directory\n",
      "rm: embedding-projector-standalone/embedding-projector-standalone-master/oss_data/minst*: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm embedding-projector-standalone/embedding-projector-standalone-master/oss_data/word2vec*\n",
    "!rm embedding-projector-standalone/embedding-projector-standalone-master/oss_data/bnmt_enjako*\n",
    "!rm embedding-projector-standalone/embedding-projector-standalone-master/oss_data/minst*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_config_with_geovec_and_iris_only = {\n",
    "  \"embeddings\": [\n",
    "    {\n",
    "      \"tensorName\":\"GeoVec\",\n",
    "      \"tensorShape\":[88626,300],\n",
    "      \"tensorPath\":\"oss_data/vecs.tsv\",\n",
    "      \"metadataPath\":\"oss_data/metadata1.tsv\"\n",
    "    },\n",
    "    {\n",
    "      \"tensorName\": \"Iris\",\n",
    "      \"tensorShape\": [150, 4],\n",
    "      \"tensorPath\": \"oss_data/iris_tensors.bytes\",\n",
    "      \"metadataPath\": \"oss_data/iris_labels.tsv\"\n",
    "    }\n",
    "  ],\n",
    "  \"modelCheckpointPath\": \"Demo datasets\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And this will change the config so geovec loads immediately on page open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('embedding-projector-standalone/embedding-projector-standalone-master/oss_data/oss_demo_projector_config.json', 'w') as fp:\n",
    "    json.dump(project_config_with_geovec_and_iris_only, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geovec",
   "language": "python",
   "name": "geovec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
